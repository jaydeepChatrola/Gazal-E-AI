{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1hHzwh-GHPDEzpsR-ipEghtDuQXklx1hG",
      "authorship_tag": "ABX9TyN4N0/0uooomcKEls33dqW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaydeepChatrola/Gazal-E-AI/blob/dev/Gazal_e_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "d5eaa068"
      },
      "source": [
        "!{sys.executable} -m pip install faiss-cpu numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wY7iGbOvTyey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login;\n",
        "login()"
      ],
      "metadata": {
        "id": "mecBC2ayVw4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "513MyfYnHam9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.bfloat16\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Wxj_RJkeUno8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"can you write poetry of mirza ghalib\"\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "WqdiyMqtWPeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = [\n",
        "    { \"role\": \"user\", \"content\": \"can you write poetry of mirza ghalib which is hazāroñ ḳhvāhisheñ aisī ki har ḳhvāhish pe dam nikle...\" },\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "V5DDjfnlXTdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c72369a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e11e62-0c65-4623-9690-41c692ecef89"
      },
      "source": [
        "dataset_path = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "all_file_paths = []\n",
        "\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        full_file_path = os.path.join(root, file)\n",
        "        all_file_paths.append(full_file_path)\n",
        "\n",
        "print(f\"Collected {len(all_file_paths)} file paths.\")"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 1345 file paths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "JdAkaGiKb0Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(all_file_paths), batch_size):\n",
        "    batch = all_file_paths[i:i+batch_size]\n",
        "\n",
        "    for file_path in batch:\n",
        "        if os.path.basename(file_path) == \".DS_Store\":\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                content = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Could not read file {file_path}: {e}\")\n",
        "            continue\n",
        "        print(f\"Processing {file_path}\")\n",
        "        documents.append({\n",
        "            'shayar': file_path.split('/')[5].replace(\"-\", \" \"),\n",
        "            'couplet': content\n",
        "        })\n",
        "\n",
        "print(f\"Collected {len(documents)} documents.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0UpuJh0hePXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee542ab3"
      },
      "source": [
        "# Task\n",
        "To convert the `documents` list to a JSON formatted string and display it, run the following code:\n",
        "\n",
        "```python\n",
        "import json\n",
        "\n",
        "# Convert the documents list to a JSON formatted string\n",
        "documents_json = json.dumps(documents, indent=4, ensure_ascii=False)\n",
        "\n",
        "# Display the JSON data\n",
        "print(documents_json)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "50cfa333"
      },
      "source": [
        "documents_json = json.dumps(documents, indent=4, ensure_ascii=False)\n",
        "print(documents_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "042505eb"
      },
      "source": [
        "import json\n",
        "\n",
        "# Assuming documents_json contains the full JSON string\n",
        "parsed_json = json.loads(documents_json)\n",
        "\n",
        "# Print the first element of the parsed JSON list\n",
        "if parsed_json:\n",
        "    print(json.dumps(parsed_json[0], indent=4, ensure_ascii=False))\n",
        "else:\n",
        "    print(\"The JSON is empty.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lines_to_couplets(lines):\n",
        "    if len(lines) % 2 != 0:\n",
        "        print(\"⚠️ Warning: odd number of lines, last line will be ignored\")\n",
        "\n",
        "    couplets = []\n",
        "    for i in range(0, len(lines) - 1, 2):\n",
        "        couplets.append(lines[i] + \"\\n\" + lines[i + 1])\n",
        "    return couplets\n",
        "\n",
        "def normalize_lines(text: str):\n",
        "    \"\"\"\n",
        "    - removes leading/trailing whitespace\n",
        "    - splits by newline\n",
        "    - removes empty lines\n",
        "    \"\"\"\n",
        "    return [\n",
        "        line.strip()\n",
        "        for line in text.strip().split(\"\\n\")\n",
        "        if line.strip()\n",
        "    ]\n",
        "\n",
        "def build_couplet_records(couplets, shayar, start_index=1):\n",
        "    records = []\n",
        "    for idx, text in enumerate(couplets, start=start_index):\n",
        "        records.append({\n",
        "            \"id\": f\"{shayar.replace(' ', '_')}_{idx:04d}\",\n",
        "            \"shayar\": shayar,\n",
        "            \"couplet_index\": idx,\n",
        "            \"text\": text\n",
        "        })\n",
        "    return records\n",
        "\n",
        "\n",
        "def process_poem_object(poem_obj, start_index=1):\n",
        "    shayar = poem_obj[\"shayar\"]\n",
        "    raw_text = poem_obj[\"couplet\"]\n",
        "\n",
        "    lines = normalize_lines(raw_text)\n",
        "    couplets = lines_to_couplets(lines)\n",
        "\n",
        "    return build_couplet_records(couplets, shayar, start_index)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Der4wfMqj9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(parsed_json, list):\n",
        "    final_dataset = []\n",
        "    counter = 1\n",
        "\n",
        "    for poem in parsed_json:\n",
        "        records = process_poem_object(poem, start_index=counter)\n",
        "        final_dataset.extend(records)\n",
        "        counter += len(records)\n",
        "\n",
        "    print(len(final_dataset))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUG4ZbIbrhKu",
        "outputId": "fd40a2e0-7fc0-4fbf-9649-c961e3105543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Warning: odd number of lines, last line will be ignored\n",
            "10501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"urdu_sher.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(final_dataset, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "e4pPfkeGstcd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}